{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01: Building a Transaction Data Pipeline\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "A real data integration pipeline that:\n",
    "- Reads transaction data from PostgreSQL\n",
    "- Filters transactions based on amount\n",
    "- Writes to MySQL database AND CSV file\n",
    "\n",
    "**Pipeline Flow:**\n",
    "```\n",
    "PostgreSQL → Peek → Filter → Transformer → MySQL\n",
    "                                         └→ CSV File\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Configure database connections\n",
    "- Build multi-stage flows\n",
    "- Route data to multiple destinations\n",
    "- Define schemas and field mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup and Configuration\n",
    "\n",
    "### Step 1.1: (Optional) Reinstall SDK\n",
    "\n",
    "Only run this if you're starting fresh or having issues. **Skip if you just completed Lab 00.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run ONLY if needed:\n",
    "# import sys\n",
    "# !{sys.executable} -m pip uninstall ibm_watsonx_data_integration -y -q\n",
    "# !{sys.executable} -m pip install ibm_watsonx_data_integration --force-reinstall -q\n",
    "# print(\"✓ SDK reinstalled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_data_integration import *\n",
    "from ibm_watsonx_data_integration.common.auth import IAMAuthenticator\n",
    "from ibm_watsonx_data_integration.services.datastage import *\n",
    "from ibm_watsonx_data_integration.services.datastage.models.enums import SEQUENTIALFILE\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Set Credentials\n",
    "\n",
    "**Use the same credentials from Lab 00.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials set (not displayed for security)\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"qSOipvfdwXJjHSWxIgVd7u0d2V2WHsOR9A_V1SntP762\"\n",
    "PROJECT_ID = \"a08157b8-d177-456f-82a3-7084576ff59e\"\n",
    "\n",
    "print(\"Credentials set (not displayed for security)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: Connect to Platform and Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to project: Testing\n"
     ]
    }
   ],
   "source": [
    "auth = IAMAuthenticator(api_key=API_KEY, base_auth_url=\"https://cloud.ibm.com\")\n",
    "platform = Platform(auth, base_api_url=\"https://api.ca-tor.dai.cloud.ibm.com\")\n",
    "project = platform.projects.get(guid=PROJECT_ID)\n",
    "\n",
    "print(f\"✓ Connected to project: {project.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Set Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Parameters:\n",
      "  Flow Name:    TransactionsLowValue-SDK\n",
      "  Filter:       amount > 25\n",
      "  MySQL Table:  processed_transactions\n",
      "  CSV File:     low_value_transactions_20251006_142326.csv\n",
      "\n",
      "✓ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Generate unique timestamp for CSV filename (prevents overwrite errors)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "FLOW_NAME = \"TransactionsLowValue-SDK\"\n",
    "FILTER_CONDITION = \"amount > 25\"\n",
    "MYSQL_TABLE = \"processed_transactions\"\n",
    "CSV_FILENAME = f\"low_value_transactions_{timestamp}.csv\"\n",
    "\n",
    "print(\"Pipeline Parameters:\")\n",
    "print(f\"  Flow Name:    {FLOW_NAME}\")\n",
    "print(f\"  Filter:       {FILTER_CONDITION}\")\n",
    "print(f\"  MySQL Table:  {MYSQL_TABLE}\")\n",
    "print(f\"  CSV File:     {CSV_FILENAME}\")\n",
    "print(\"\\n✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Step 1.6: Delete Existing Flow\n",
    "\n",
    "**Only run this if you want to delete a previous version of this flow.**\n",
    "\n",
    "Useful if you're re-running the lab and want to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the existing flow before creating a new one:\n",
    "# try:\n",
    "#     duplicate = project.flows.get(name=FLOW_NAME)\n",
    "#     project.delete_flow(duplicate)\n",
    "#     print(f\"✓ Deleted existing flow: {FLOW_NAME}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Flow not found or already deleted: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Build Pipeline Graph\n",
    "\n",
    "Creates all stages, connects them, and defines schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created flow: TransactionsLowValue-SDK\n",
      "\n",
      "✓ Added PostgreSQL source\n",
      "✓ Added Peek_1\n",
      "✓ Added Filter (amount > 25)\n",
      "✓ Added Transformer\n",
      "✓ Added MySQL target (processed_transactions)\n",
      "✓ Added Peek_2\n",
      "✓ Added CSV target (low_value_transactions_20251006_142326.csv)\n",
      "\n",
      "✓ Connected all stages\n",
      "\n",
      "✓ Defined schemas on all links\n",
      "\n",
      "✓ Pipeline graph complete!\n"
     ]
    }
   ],
   "source": [
    "# Create flow\n",
    "flow = project.create_flow(name=FLOW_NAME, environment=None, flow_type=\"datastage\")\n",
    "print(f\"✓ Created flow: {FLOW_NAME}\\n\")\n",
    "\n",
    "# Add PostgreSQL source\n",
    "transactions_1 = flow.add_stage(\"PostgreSQL\", \"transactions_1\")\n",
    "transactions_1.configuration.runtime_column_propagation = False\n",
    "transactions_1.configuration.table_name = \"transactions\"\n",
    "transactions_1.configuration.connection.name = \"PostgreSQL_conn\"\n",
    "transactions_1.configuration.connection.database = \"cpd\"\n",
    "transactions_1.configuration.connection.defer_credentials = False\n",
    "transactions_1.configuration.connection.hostname_or_ip_address = \"52.116.198.152\"\n",
    "transactions_1.configuration.connection.password = \"DataDuck!\"\n",
    "transactions_1.configuration.connection.port = \"5432\"\n",
    "transactions_1.configuration.connection.proxy = False\n",
    "transactions_1.configuration.connection.port_is_ssl_enabled = False\n",
    "transactions_1.configuration.connection.username = \"cpd\"\n",
    "print(\"✓ Added PostgreSQL source\")\n",
    "\n",
    "# Add Peek (monitoring)\n",
    "peek_1 = flow.add_stage(\"Peek\", \"Peek_1\")\n",
    "peek_1.configuration.runtime_column_propagation = False\n",
    "peek_1.configuration.outputlink_ordering_list = [{\"link_label\": \"Output 1\", \"link_name\": \"Link_2\"}]\n",
    "print(\"✓ Added Peek_1\")\n",
    "\n",
    "# Add Filter\n",
    "filter_1 = flow.add_stage(\"Filter\", \"Filter_1\")\n",
    "filter_1.configuration.show_coll_type = False\n",
    "filter_1.configuration.show_part_type = True\n",
    "filter_1.configuration.show_sort_options = False\n",
    "filter_1.configuration.where_properties = [{\"where\": FILTER_CONDITION, \"target\": \"0\"}]\n",
    "print(f\"✓ Added Filter ({FILTER_CONDITION})\")\n",
    "\n",
    "# Add Transformer\n",
    "transformer_1 = flow.add_stage(\"Transformer\", \"Transformer_1\")\n",
    "print(\"✓ Added Transformer\")\n",
    "\n",
    "# Add MySQL target\n",
    "tm_ds_db_1_1 = flow.add_stage(\"MySQL\", \"TM_DS_DB_1_1\")\n",
    "tm_ds_db_1_1.configuration.column_metadata_change_propagation = False\n",
    "tm_ds_db_1_1.configuration.output_acp_should_hide = False\n",
    "tm_ds_db_1_1.configuration.schema_name = \"TM_DS_DB_1\"\n",
    "tm_ds_db_1_1.configuration.show_coll_type = False\n",
    "tm_ds_db_1_1.configuration.show_part_type = True\n",
    "tm_ds_db_1_1.configuration.show_sort_options = False\n",
    "tm_ds_db_1_1.configuration.table_name = MYSQL_TABLE\n",
    "tm_ds_db_1_1.configuration.connection.name = \"MySQL Legacy Financial DB\"\n",
    "tm_ds_db_1_1.configuration.connection.database = \"TM_DS_DB_1\"\n",
    "tm_ds_db_1_1.configuration.connection.defer_credentials = \"false\"\n",
    "tm_ds_db_1_1.configuration.connection.hostname_or_ip_address = \"4d275b38-2eee-4b4d-8a88-cb022388e975.blijti4d0v0nkr55oei0.databases.appdomain.cloud\"\n",
    "tm_ds_db_1_1.configuration.connection.password = \"eDGxvzFX7tK_\"\n",
    "tm_ds_db_1_1.configuration.connection.port = \"32661\"\n",
    "tm_ds_db_1_1.configuration.connection.proxy = False\n",
    "tm_ds_db_1_1.configuration.connection.port_is_ssl_enabled = True\n",
    "tm_ds_db_1_1.configuration.connection.username = \"TM_DS_USER\"\n",
    "print(f\"✓ Added MySQL target ({MYSQL_TABLE})\")\n",
    "\n",
    "# Add second Peek\n",
    "peek_2 = flow.add_stage(\"Peek\", \"Peek_2\")\n",
    "peek_2.configuration.outputlink_ordering_list = [{\"link_label\": \"Output 1\", \"link_name\": \"Link_6\"}]\n",
    "print(\"✓ Added Peek_2\")\n",
    "\n",
    "# Add CSV target\n",
    "sequential_file_1 = flow.add_stage(\"Sequential file\", \"Sequential_file_1\")\n",
    "sequential_file_1.configuration.file = [f\"/ds-storage/{CSV_FILENAME}\"]\n",
    "sequential_file_1.configuration.first_line_is_column_names = SEQUENTIALFILE.FirstLineColumnNames.true\n",
    "sequential_file_1.configuration.null_field_value = \"'NULL'\"\n",
    "sequential_file_1.configuration.show_coll_type = True\n",
    "sequential_file_1.configuration.show_part_type = False\n",
    "sequential_file_1.configuration.show_sort_options = True\n",
    "print(f\"✓ Added CSV target ({CSV_FILENAME})\\n\")\n",
    "\n",
    "# Connect stages\n",
    "link_1 = transactions_1.connect_output_to(peek_1)\n",
    "link_1.name = \"Link_1\"\n",
    "link_2 = peek_1.connect_output_to(filter_1)\n",
    "link_2.name = \"Link_2\"\n",
    "link_3 = filter_1.connect_output_to(transformer_1)\n",
    "link_3.name = \"Link_3\"\n",
    "link_4 = transformer_1.connect_output_to(tm_ds_db_1_1)\n",
    "link_4.name = \"Link_4\"\n",
    "link_5 = transformer_1.connect_output_to(peek_2)\n",
    "link_5.name = \"Link_5\"\n",
    "link_6 = peek_2.connect_output_to(sequential_file_1)\n",
    "link_6.name = \"Link_6\"\n",
    "print(\"✓ Connected all stages\\n\")\n",
    "\n",
    "# Define schemas\n",
    "# Link 1: PostgreSQL → Peek_1\n",
    "schema_1 = link_1.create_schema()\n",
    "schema_1.add_field(\"INTEGER\", \"id\")\n",
    "schema_1.add_field(\"INTEGER\", \"account_id\")\n",
    "schema_1.add_field(\"VARCHAR\", \"timestamp\").length(50)\n",
    "schema_1.add_field(\"NUMERIC\", \"amount\").length(10).scale(2)\n",
    "schema_1.add_field(\"LONGVARCHAR\", \"location\").length(1024)\n",
    "\n",
    "# Link 2: Peek_1 → Filter\n",
    "schema_2 = link_2.create_schema()\n",
    "schema_2.add_field(\"INTEGER\", \"id\").source(\"Link_1.id\")\n",
    "schema_2.add_field(\"INTEGER\", \"account_id\").source(\"Link_1.account_id\")\n",
    "schema_2.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_1.timestamp\").length(50)\n",
    "schema_2.add_field(\"NUMERIC\", \"amount\").source(\"Link_1.amount\").length(10).scale(2)\n",
    "schema_2.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_1.location\").length(1024)\n",
    "\n",
    "# Link 3: Filter → Transformer\n",
    "schema_3 = link_3.create_schema()\n",
    "schema_3.add_field(\"INTEGER\", \"id\").source(\"Link_2.id\")\n",
    "schema_3.add_field(\"INTEGER\", \"account_id\").source(\"Link_2.account_id\")\n",
    "schema_3.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_2.timestamp\").length(50)\n",
    "schema_3.add_field(\"NUMERIC\", \"amount\").source(\"Link_2.amount\").length(10).scale(2)\n",
    "schema_3.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_2.location\").length(1024)\n",
    "\n",
    "# Link 4: Transformer → MySQL (removes id)\n",
    "schema_4 = link_4.create_schema()\n",
    "schema_4.add_field(\"INTEGER\", \"account_id\").source(\"Link_3.account_id\")\n",
    "schema_4.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_3.timestamp\").length(50)\n",
    "schema_4.add_field(\"NUMERIC\", \"amount\").source(\"Link_3.amount\").length(10).scale(2)\n",
    "schema_4.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_3.location\").length(1024)\n",
    "\n",
    "# Link 5: Transformer → Peek_2 (keeps all fields)\n",
    "schema_5 = link_5.create_schema()\n",
    "schema_5.add_field(\"INTEGER\", \"id\").source(\"Link_3.id\")\n",
    "schema_5.add_field(\"INTEGER\", \"account_id\").source(\"Link_3.account_id\")\n",
    "schema_5.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_3.timestamp\").length(50)\n",
    "schema_5.add_field(\"NUMERIC\", \"amount\").source(\"Link_3.amount\").length(10).scale(2)\n",
    "schema_5.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_3.location\").length(1024)\n",
    "\n",
    "# Link 6: Peek_2 → CSV\n",
    "schema_6 = link_6.create_schema()\n",
    "schema_6.add_field(\"INTEGER\", \"id\").source(\"Link_5.id\")\n",
    "schema_6.add_field(\"INTEGER\", \"account_id\").source(\"Link_5.account_id\")\n",
    "schema_6.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_5.timestamp\").length(50)\n",
    "schema_6.add_field(\"NUMERIC\", \"amount\").source(\"Link_5.amount\").length(10).scale(2)\n",
    "schema_6.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_5.location\").length(1024)\n",
    "\n",
    "print(\"✓ Defined schemas on all links\")\n",
    "print(\"\\n✓ Pipeline graph complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Save and Execute\n",
    "\n",
    "Saves the flow and creates a job to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flow saved: TransactionsLowValue-SDK\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaeldobson/Documents/Product Management/SDK_notebooks/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.ca-tor.dai.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Job created: TransactionsLowValue-SDK_job\n",
      "✓ Job started\n",
      "\n",
      "============================================================\n",
      "SUCCESS! Pipeline is running.\n",
      "============================================================\n",
      "\n",
      "What's happening now:\n",
      "  1. Reading from PostgreSQL (transactions table)\n",
      "  2. Filtering: amount > 25\n",
      "  3. Writing to MySQL: processed_transactions\n",
      "  4. Writing to CSV: low_value_transactions_20251006_142326.csv\n",
      "\n",
      "Next: Go to the UI to verify (see lab guide)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save flow\n",
    "project.update_flow(flow)\n",
    "print(f\"✓ Flow saved: {FLOW_NAME}\\n\")\n",
    "\n",
    "# Create and start job\n",
    "job = project.create_job(name=f\"{FLOW_NAME}_job\", flow=flow)\n",
    "print(f\"✓ Job created: {FLOW_NAME}_job\")\n",
    "\n",
    "job_run = job.start(name=f\"{FLOW_NAME} job run\", description=\"Lab 01\")\n",
    "print(f\"✓ Job started\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUCCESS! Pipeline is running.\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nWhat's happening now:\")\n",
    "print(f\"  1. Reading from PostgreSQL (transactions table)\")\n",
    "print(f\"  2. Filtering: {FILTER_CONDITION}\")\n",
    "print(f\"  3. Writing to MySQL: {MYSQL_TABLE}\")\n",
    "print(f\"  4. Writing to CSV: {CSV_FILENAME}\")\n",
    "print(\"\\nNext: Go to the UI to verify (see lab guide)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What You Built:\n",
    "- **6-stage pipeline**: PostgreSQL → Peek → Filter → Transformer → MySQL + CSV\n",
    "- **Database connections**: Configured PostgreSQL source and MySQL target\n",
    "- **Data routing**: Single source to dual destinations\n",
    "- **Schema definitions**: Complete field mappings across all links\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Flow creation**: `project.create_flow()`\n",
    "2. **Stage configuration**: Each stage has specific properties\n",
    "3. **Link connections**: `stage1.connect_output_to(stage2)`\n",
    "4. **Schema mapping**: `.source()` tracks data lineage\n",
    "5. **Job execution**: `job.start()` runs the pipeline\n",
    "\n",
    "### The Pattern:\n",
    "```python\n",
    "1. Create flow\n",
    "2. Add stages\n",
    "3. Connect stages\n",
    "4. Define schemas\n",
    "5. Save and execute\n",
    "```\n",
    "\n",
    "This pattern applies to **any** DataStage pipeline you build with the SDK.\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Lab 02\n",
    "\n",
    "You just built **one** pipeline manually.\n",
    "\n",
    "Lab 02 shows you how to create **multiple pipelines** automatically using parameters and loops.\n",
    "\n",
    "**Preview**: 3 pipeline variations (low/medium/high value) in under 5 minutes.\n",
    "\n",
    "That's the power of SDK automation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
