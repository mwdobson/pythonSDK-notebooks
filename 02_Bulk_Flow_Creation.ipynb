{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Bulk Flow Creation with Templates\n",
    "\n",
    "**Objective:** Learn to automate data pipeline creation by using templates and loops to generate multiple flow variations from a single pattern.\n",
    "\n",
    "**What you'll build:** Three transaction processing pipelines (Low/Medium/High value) created programmatically in under 5 minutes - a task that would take approximately 60 minutes manually in the UI.\n",
    "\n",
    "**Key Concept:** Template-based automation demonstrates the true power of the SDK for production-scale data engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: (Optional) SDK Reinstall\n",
    "\n",
    "**Skip this cell if you just completed Lab 01.** Only uncomment and run if you're starting a fresh session or experiencing import issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below ONLY if you need to reinstall the SDK\n",
    "# !pip uninstall ibm_watsonx_data_integration -y\n",
    "# !pip install ibm_watsonx_data_integration --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_data_integration import *\n",
    "from ibm_watsonx_data_integration.common.auth import IAMAuthenticator\n",
    "from ibm_watsonx_data_integration.services.datastage import *\n",
    "from ibm_watsonx_data_integration.services.datastage.models.enums import SEQUENTIALFILE\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Set Your Credentials\n",
    "\n",
    "**Use the same API key and Project ID from Lab 00 and Lab 01.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"YOUR_IBM_CLOUD_API_KEY\"\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"\n",
    "\n",
    "print(\"Credentials set (not displayed for security)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Connect to Platform and Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "auth = IAMAuthenticator(api_key=API_KEY, base_auth_url=\"https://cloud.ibm.com\")\n",
    "print(\"Authenticator created\")\n",
    "\n",
    "# Connect to platform\n",
    "platform = Platform(auth, base_api_url=\"https://api.ca-tor.dai.cloud.ibm.com\")\n",
    "print(\"Platform connection initialized\")\n",
    "\n",
    "# Get project\n",
    "project = platform.projects.get(guid=PROJECT_ID)\n",
    "print(f\"Connected to project: {project.name}\")\n",
    "print(f\"Project ID: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Define Flow Parameters\n",
    "\n",
    "**Key Concept:** This parameters list defines three different pipeline variations. Each will filter transactions at a different threshold and write to uniquely-named CSV files.\n",
    "\n",
    "**Note:** Flow names use \"Templated\" prefix to avoid conflicts with Lab 01 flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate timestamp for unique CSV filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define three flow variations\n",
    "FLOW_PARAMETERS = [\n",
    "    {\n",
    "        \"flow_name\": \"TemplatedTransactionsLowValue-SDK\",\n",
    "        \"filter_condition\": \"amount > 25\",\n",
    "        \"mysql_table\": \"processed_transactions\",\n",
    "        \"csv_filename\": f\"templated_low_value_{timestamp}.csv\"\n",
    "    },\n",
    "    {\n",
    "        \"flow_name\": \"TemplatedTransactionsMediumValue-SDK\",\n",
    "        \"filter_condition\": \"amount > 100\",\n",
    "        \"mysql_table\": \"processed_transactions\",\n",
    "        \"csv_filename\": f\"templated_medium_value_{timestamp}.csv\"\n",
    "    },\n",
    "    {\n",
    "        \"flow_name\": \"TemplatedTransactionsHighValue-SDK\",\n",
    "        \"filter_condition\": \"amount > 750\",\n",
    "        \"mysql_table\": \"processed_transactions\",\n",
    "        \"csv_filename\": f\"templated_high_value_{timestamp}.csv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(FLOW_PARAMETERS)} flow variations\")\n",
    "print(f\"Timestamp: {timestamp}\")\n",
    "for params in FLOW_PARAMETERS:\n",
    "    print(f\"  - {params['flow_name']}: {params['filter_condition']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: (Optional) Delete Existing Flows\n",
    "\n",
    "**Use this if you need to re-run the lab.** Uncomment to delete any flows with matching names before creating new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to delete existing flows before re-running\n",
    "\n",
    "# for params in FLOW_PARAMETERS:\n",
    "#     try:\n",
    "#         existing_flow = project.flows.get(name=params['flow_name'])\n",
    "#         project.delete_flow(existing_flow)\n",
    "#         print(f\"Deleted existing flow: {params['flow_name']}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Flow '{params['flow_name']}' not found or already deleted\")\n",
    "\n",
    "print(\"Delete operation complete (if uncommented)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Flow Template Function\n",
    "\n",
    "This function encapsulates the entire flow creation pattern from Lab 01, making it reusable with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Define Flow Creation Function\n",
    "\n",
    "**This is the template pattern.** The function accepts parameters and builds a complete 6-stage pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transaction_flow(flow_name, filter_condition, mysql_table, csv_filename):\n",
    "    \"\"\"\n",
    "    Create a transaction processing flow with specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        flow_name: Name for the flow\n",
    "        filter_condition: SQL-like filter condition (e.g., 'amount > 100')\n",
    "        mysql_table: Target MySQL table name\n",
    "        csv_filename: Output CSV filename\n",
    "    \n",
    "    Returns:\n",
    "        Created flow object\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating flow: {flow_name}\")\n",
    "    print(f\"  Filter: {filter_condition}\")\n",
    "    print(f\"  MySQL Table: {mysql_table}\")\n",
    "    print(f\"  CSV File: {csv_filename}\")\n",
    "    \n",
    "    # Create flow container\n",
    "    flow = project.create_flow(name=flow_name, environment=None, flow_type=\"datastage\")\n",
    "    \n",
    "    # Stage 1: PostgreSQL Source\n",
    "    transactions_1 = flow.add_stage(\"PostgreSQL\", \"transactions_1\")\n",
    "    transactions_1.configuration.runtime_column_propagation = False\n",
    "    transactions_1.configuration.table_name = \"transactions\"\n",
    "    transactions_1.configuration.connection.name = \"PostgreSQL_conn\"\n",
    "    transactions_1.configuration.connection.database = \"cpd\"\n",
    "    transactions_1.configuration.connection.defer_credentials = False\n",
    "    transactions_1.configuration.connection.hostname_or_ip_address = \"52.116.198.152\"\n",
    "    transactions_1.configuration.connection.password = \"DataDuck!\"\n",
    "    transactions_1.configuration.connection.port = \"5432\"\n",
    "    transactions_1.configuration.connection.proxy = False\n",
    "    transactions_1.configuration.connection.port_is_ssl_enabled = False\n",
    "    transactions_1.configuration.connection.username = \"cpd\"\n",
    "    \n",
    "    # Stage 2: Peek (monitor input)\n",
    "    peek_1 = flow.add_stage(\"Peek\", \"Peek_1\")\n",
    "    peek_1.configuration.runtime_column_propagation = False\n",
    "    peek_1.configuration.outputlink_ordering_list = [{\"link_label\": \"Output 1\", \"link_name\": \"Link_2\"}]\n",
    "    \n",
    "    # Stage 3: Filter (parameterized condition)\n",
    "    filter_1 = flow.add_stage(\"Filter\", \"Filter_1\")\n",
    "    filter_1.configuration.show_coll_type = False\n",
    "    filter_1.configuration.show_part_type = True\n",
    "    filter_1.configuration.show_sort_options = False\n",
    "    filter_1.configuration.where_properties = [{\"where\": filter_condition, \"target\": \"0\"}]\n",
    "    \n",
    "    # Stage 4: Transformer (reshape and route)\n",
    "    transformer_1 = flow.add_stage(\"Transformer\", \"Transformer_1\")\n",
    "    \n",
    "    # Stage 5: MySQL Target\n",
    "    tm_ds_db_1_1 = flow.add_stage(\"MySQL\", \"TM_DS_DB_1_1\")\n",
    "    tm_ds_db_1_1.configuration.column_metadata_change_propagation = False\n",
    "    tm_ds_db_1_1.configuration.output_acp_should_hide = False\n",
    "    tm_ds_db_1_1.configuration.schema_name = \"TM_DS_DB_1\"\n",
    "    tm_ds_db_1_1.configuration.show_coll_type = False\n",
    "    tm_ds_db_1_1.configuration.show_part_type = True\n",
    "    tm_ds_db_1_1.configuration.show_sort_options = False\n",
    "    tm_ds_db_1_1.configuration.table_name = mysql_table\n",
    "    tm_ds_db_1_1.configuration.connection.name = \"MySQL Legacy Financial DB\"\n",
    "    tm_ds_db_1_1.configuration.connection.database = \"TM_DS_DB_1\"\n",
    "    tm_ds_db_1_1.configuration.connection.defer_credentials = \"false\"\n",
    "    tm_ds_db_1_1.configuration.connection.hostname_or_ip_address = \"4d275b38-2eee-4b4d-8a88-cb022388e975.blijti4d0v0nkr55oei0.databases.appdomain.cloud\"\n",
    "    tm_ds_db_1_1.configuration.connection.password = \"eDGxvzFX7tK_\"\n",
    "    tm_ds_db_1_1.configuration.connection.port = \"32661\"\n",
    "    tm_ds_db_1_1.configuration.connection.proxy = False\n",
    "    tm_ds_db_1_1.configuration.connection.port_is_ssl_enabled = True\n",
    "    tm_ds_db_1_1.configuration.connection.username = \"TM_DS_USER\"\n",
    "    \n",
    "    # Stage 6a: Peek (monitor before CSV)\n",
    "    peek_2 = flow.add_stage(\"Peek\", \"Peek_2\")\n",
    "    peek_2.configuration.outputlink_ordering_list = [{\"link_label\": \"Output 1\", \"link_name\": \"Link_6\"}]\n",
    "    \n",
    "    # Stage 6b: Sequential File (CSV output)\n",
    "    sequential_file_1 = flow.add_stage(\"Sequential file\", \"Sequential_file_1\")\n",
    "    sequential_file_1.configuration.file = [f\"/ds-storage/{csv_filename}\"]\n",
    "    sequential_file_1.configuration.first_line_is_column_names = SEQUENTIALFILE.FirstLineColumnNames.true\n",
    "    sequential_file_1.configuration.null_field_value = \"'NULL'\"\n",
    "    sequential_file_1.configuration.show_coll_type = True\n",
    "    sequential_file_1.configuration.show_part_type = False\n",
    "    sequential_file_1.configuration.show_sort_options = True\n",
    "    \n",
    "    # Connect all stages (create the graph)\n",
    "    link_1 = transactions_1.connect_output_to(peek_1)\n",
    "    link_1.name = \"Link_1\"\n",
    "    \n",
    "    link_2 = peek_1.connect_output_to(filter_1)\n",
    "    link_2.name = \"Link_2\"\n",
    "    \n",
    "    link_3 = filter_1.connect_output_to(transformer_1)\n",
    "    link_3.name = \"Link_3\"\n",
    "    \n",
    "    link_4 = transformer_1.connect_output_to(tm_ds_db_1_1)\n",
    "    link_4.name = \"Link_4\"\n",
    "    \n",
    "    link_5 = transformer_1.connect_output_to(peek_2)\n",
    "    link_5.name = \"Link_5\"\n",
    "    \n",
    "    link_6 = peek_2.connect_output_to(sequential_file_1)\n",
    "    link_6.name = \"Link_6\"\n",
    "    \n",
    "    # Define schemas on all links\n",
    "    \n",
    "    # Link 1: PostgreSQL to Peek_1\n",
    "    transactions_1_schema = link_1.create_schema()\n",
    "    transactions_1_schema.add_field(\"INTEGER\", \"id\")\n",
    "    transactions_1_schema.add_field(\"INTEGER\", \"account_id\")\n",
    "    transactions_1_schema.add_field(\"VARCHAR\", \"timestamp\").length(50)\n",
    "    transactions_1_schema.add_field(\"NUMERIC\", \"amount\").length(10).scale(2)\n",
    "    transactions_1_schema.add_field(\"LONGVARCHAR\", \"location\").length(1024)\n",
    "    \n",
    "    # Link 2: Peek_1 to Filter\n",
    "    peek_1_schema = link_2.create_schema()\n",
    "    peek_1_schema.add_field(\"INTEGER\", \"id\").source(\"Link_1.id\")\n",
    "    peek_1_schema.add_field(\"INTEGER\", \"account_id\").source(\"Link_1.account_id\")\n",
    "    peek_1_schema.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_1.timestamp\").length(50)\n",
    "    peek_1_schema.add_field(\"NUMERIC\", \"amount\").source(\"Link_1.amount\").length(10).scale(2)\n",
    "    peek_1_schema.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_1.location\").length(1024)\n",
    "    \n",
    "    # Link 3: Filter to Transformer\n",
    "    filter_1_schema = link_3.create_schema()\n",
    "    filter_1_schema.add_field(\"INTEGER\", \"id\").source(\"Link_2.id\")\n",
    "    filter_1_schema.add_field(\"INTEGER\", \"account_id\").source(\"Link_2.account_id\")\n",
    "    filter_1_schema.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_2.timestamp\").length(50)\n",
    "    filter_1_schema.add_field(\"NUMERIC\", \"amount\").source(\"Link_2.amount\").length(10).scale(2)\n",
    "    filter_1_schema.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_2.location\").length(1024)\n",
    "    \n",
    "    # Link 4: Transformer to MySQL (removes id field for auto-increment)\n",
    "    transformer_1_schema = link_4.create_schema()\n",
    "    transformer_1_schema.add_field(\"INTEGER\", \"account_id\").source(\"Link_3.account_id\")\n",
    "    transformer_1_schema.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_3.timestamp\").length(50)\n",
    "    transformer_1_schema.add_field(\"NUMERIC\", \"amount\").source(\"Link_3.amount\").length(10).scale(2)\n",
    "    transformer_1_schema.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_3.location\").length(1024)\n",
    "    \n",
    "    # Link 5: Transformer to Peek_2 (keeps all fields)\n",
    "    transformer_1_schema_2 = link_5.create_schema()\n",
    "    transformer_1_schema_2.add_field(\"INTEGER\", \"id\").source(\"Link_3.id\")\n",
    "    transformer_1_schema_2.add_field(\"INTEGER\", \"account_id\").source(\"Link_3.account_id\")\n",
    "    transformer_1_schema_2.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_3.timestamp\").length(50)\n",
    "    transformer_1_schema_2.add_field(\"NUMERIC\", \"amount\").source(\"Link_3.amount\").length(10).scale(2)\n",
    "    transformer_1_schema_2.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_3.location\").length(1024)\n",
    "    \n",
    "    # Link 6: Peek_2 to CSV\n",
    "    peek_2_schema = link_6.create_schema()\n",
    "    peek_2_schema.add_field(\"INTEGER\", \"id\").source(\"Link_5.id\")\n",
    "    peek_2_schema.add_field(\"INTEGER\", \"account_id\").source(\"Link_5.account_id\")\n",
    "    peek_2_schema.add_field(\"VARCHAR\", \"timestamp\").source(\"Link_5.timestamp\").length(50)\n",
    "    peek_2_schema.add_field(\"NUMERIC\", \"amount\").source(\"Link_5.amount\").length(10).scale(2)\n",
    "    peek_2_schema.add_field(\"LONGVARCHAR\", \"location\").source(\"Link_5.location\").length(1024)\n",
    "    \n",
    "    print(f\"  Flow structure created with 6 stages and 6 links\")\n",
    "    \n",
    "    return flow\n",
    "\n",
    "print(\"Flow creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create and Execute Flows\n",
    "\n",
    "**This is where the automation happens.** Watch as we create three complete pipelines with a simple loop.\n",
    "\n",
    "**Note:** You may see \"unverified HTTPS request\" warnings - these are normal and safe to ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Create All Flows Using the Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CREATING FLOWS - This would take approximately 60 minutes manually\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "created_flows = []\n",
    "\n",
    "for i, params in enumerate(FLOW_PARAMETERS, 1):\n",
    "    print(f\"\\n[{i}/{len(FLOW_PARAMETERS)}] Processing: {params['flow_name']}\")\n",
    "    \n",
    "    # Create the flow using our template function\n",
    "    flow = create_transaction_flow(\n",
    "        flow_name=params['flow_name'],\n",
    "        filter_condition=params['filter_condition'],\n",
    "        mysql_table=params['mysql_table'],\n",
    "        csv_filename=params['csv_filename']\n",
    "    )\n",
    "    \n",
    "    # Save the flow to watsonx.data Integration\n",
    "    project.update_flow(flow)\n",
    "    print(f\"  Flow saved to watsonx.data Integration\")\n",
    "    \n",
    "    # Store for job creation\n",
    "    created_flows.append({\n",
    "        'flow': flow,\n",
    "        'name': params['flow_name']\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"SUCCESS! Created {len(created_flows)} flows in seconds\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFlows created:\")\n",
    "for flow_info in created_flows:\n",
    "    print(f\"  - {flow_info['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Create and Execute Jobs for All Flows\n",
    "\n",
    "**Note:** Jobs will run concurrently. Execution typically takes 1-2 minutes total for all three jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING AND EXECUTING JOBS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "job_runs = []\n",
    "\n",
    "for i, flow_info in enumerate(created_flows, 1):\n",
    "    flow = flow_info['flow']\n",
    "    flow_name = flow_info['name']\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(created_flows)}] {flow_name}\")\n",
    "    \n",
    "    # Create job\n",
    "    job = project.create_job(\n",
    "        name=f\"{flow_name}_job\",\n",
    "        flow=flow\n",
    "    )\n",
    "    print(f\"  Job created: {flow_name}_job\")\n",
    "    \n",
    "    # Execute job\n",
    "    job_run = job.start(\n",
    "        name=f\"{flow_name} job run\",\n",
    "        description=\"Lab 02 - Bulk flow creation\"\n",
    "    )\n",
    "    print(f\"  Job started\")\n",
    "    \n",
    "    job_runs.append({\n",
    "        'flow_name': flow_name,\n",
    "        'job': job,\n",
    "        'job_run': job_run\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ALL JOBS STARTED! {len(job_runs)} pipelines now processing data\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nJobs executing:\")\n",
    "for run_info in job_runs:\n",
    "    print(f\"  - {run_info['flow_name']}_job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Summary and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Lab Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAB 02 COMPLETE - WHAT YOU ACCOMPLISHED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTHE NUMBERS:\")\n",
    "print(f\"  Flows created: {len(created_flows)}\")\n",
    "print(f\"  Jobs executed: {len(job_runs)}\")\n",
    "print(f\"  Total stages configured: {len(created_flows) * 6}\")\n",
    "print(f\"  Total links defined: {len(created_flows) * 6}\")\n",
    "print(f\"  Execution time: approximately 2-3 minutes\")\n",
    "print(f\"  Manual UI time would be: approximately 60 minutes\")\n",
    "\n",
    "print(\"\\nWHAT YOU LEARNED:\")\n",
    "print(\"  - Template-based flow creation pattern\")\n",
    "print(\"  - Parameterization for reusability\")\n",
    "print(\"  - Loop-based bulk operations\")\n",
    "print(\"  - Production automation workflows\")\n",
    "print(\"  - The power of SDK vs. manual UI work\")\n",
    "\n",
    "print(\"\\nVERIFY YOUR WORK:\")\n",
    "print(\"  1. Go to watsonx.data Integration UI:\")\n",
    "print(\"     https://ca-tor.dai.cloud.ibm.com/df/home?context=df\")\n",
    "print(\"  2. Navigate to Jobs tab\")\n",
    "print(\"     Confirm all 3 jobs are running/completed\")\n",
    "print(\"  3. Navigate to Assets tab\")\n",
    "print(\"     Confirm all 3 flows exist:\")\n",
    "for flow_info in created_flows:\n",
    "    print(f\"       - {flow_info['name']}\")\n",
    "\n",
    "print(\"\\nNEXT STEP: Lab 03 - Real-time Streaming Pipeline\")\n",
    "print(\"  Learn to process continuous data streams with:\")\n",
    "print(\"  - Kafka message consumption\")\n",
    "print(\"  - Real-time fraud detection logic\")\n",
    "print(\"  - Dual streaming outputs (Elasticsearch + SingleStore)\")\n",
    "print(\"  - Continuous pipeline monitoring\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab Environment",
   "language": "python",
   "name": "lab-env-brew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
