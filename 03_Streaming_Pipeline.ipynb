{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Real-time Streaming Pipeline\n",
    "\n",
    "**Objective:** Build a continuous data streaming pipeline that processes credit card transactions in real-time to detect fraudulent activity.\n",
    "\n",
    "**What you'll build:** A fraud detection system that:\n",
    "- Consumes live transaction data from Kafka\n",
    "- Calculates transaction velocity (distance / time between transactions)\n",
    "- Flags suspicious transactions in real-time\n",
    "- Writes to dual destinations for monitoring and storage\n",
    "\n",
    "**Key Concept:** Streaming flows process data continuously, unlike batch flows that run on schedule. This enables real-time decision making and immediate responses to events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Streaming vs. Batch\n",
    "\n",
    "**Batch Processing (Labs 1-2):**\n",
    "- Processes data in scheduled chunks\n",
    "- Example: Nightly data warehouse loads\n",
    "- Use when: Historical analysis, bulk transformations, periodic updates\n",
    "\n",
    "**Streaming Processing (This Lab):**\n",
    "- Processes data continuously as it arrives\n",
    "- Example: Real-time fraud detection, live dashboards\n",
    "- Use when: Immediate action required, event-driven architectures\n",
    "\n",
    "**This lab demonstrates streaming with a fraud detection use case: detecting impossible travel patterns (transactions from locations too far apart given the time elapsed).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: (Optional) SDK Reinstall\n",
    "\n",
    "**Skip this if you just completed Labs 1-2.** Only uncomment if starting a fresh session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below ONLY if you need to reinstall the SDK\n",
    "# !pip uninstall ibm_watsonx_data_integration -y\n",
    "# !pip install ibm_watsonx_data_integration --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_data_integration import Platform\n",
    "from ibm_watsonx_data_integration.common.auth import IAMAuthenticator\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Set Your Credentials\n",
    "\n",
    "**Use the same API key and Project ID from previous labs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"YOUR_IBM_CLOUD_API_KEY\"\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"\n",
    "\n",
    "print(\"Credentials set (not displayed for security)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Connect to Platform and Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "auth = IAMAuthenticator(API_KEY)\n",
    "print(\"Authenticator created\")\n",
    "\n",
    "# Connect to platform\n",
    "platform = Platform(auth, base_api_url=\"https://api.ca-tor.dai.cloud.ibm.com\")\n",
    "print(\"Platform connection initialized\")\n",
    "\n",
    "# Get project\n",
    "project = platform.projects.get(guid=PROJECT_ID)\n",
    "print(f\"Connected to project: {project.name}\")\n",
    "print(f\"Project ID: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Define Flow Parameters\n",
    "\n",
    "**Key Parameters:**\n",
    "- **Flow Name:** Unique identifier for this streaming flow\n",
    "- **Kafka Topic:** Source of credit card transaction data\n",
    "- **Velocity Threshold:** Speed in m/s that flags suspicious activity (268.224 m/s ≈ speed of light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming flow configuration\n",
    "FLOW_NAME = \"StreamingFraudDetection-SDK\"\n",
    "KAFKA_TOPIC = \"credit_card_transactions\"\n",
    "KAFKA_BROKER = \"kafka:9092\"\n",
    "VELOCITY_THRESHOLD = 268.224\n",
    "\n",
    "print(f\"Flow Name: {FLOW_NAME}\")\n",
    "print(f\"Kafka Topic: {KAFKA_TOPIC}\")\n",
    "print(f\"Kafka Broker: {KAFKA_BROKER}\")\n",
    "print(f\"Velocity Threshold: {VELOCITY_THRESHOLD} m/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: (Optional) Delete Existing Flow\n",
    "\n",
    "**Use this if you need to re-run the lab.** Uncomment to delete the flow before recreating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to delete existing flow before re-running\n",
    "\n",
    "# try:\n",
    "#     existing_flow = project.flows.get(name=FLOW_NAME)\n",
    "#     project.delete_flow(existing_flow)\n",
    "#     print(f\"Deleted existing flow: {FLOW_NAME}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Flow '{FLOW_NAME}' not found or already deleted\")\n",
    "\n",
    "print(\"Delete operation complete (if uncommented)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build Streaming Flow\n",
    "\n",
    "**Streaming Flow Architecture:**\n",
    "\n",
    "```\n",
    "Kafka Consumer\n",
    "    ↓\n",
    "JDBC Lookup (calculate velocity from previous transaction)\n",
    "    ↓\n",
    "Expression Evaluator (flag if velocity > threshold)\n",
    "    ↓         ↓\n",
    "Elasticsearch  SingleStore\n",
    "(monitoring)   (storage)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Create Streaming Flow with All Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating streaming flow...\")\n",
    "\n",
    "# Create flow container\n",
    "flow = project.create_flow(name=FLOW_NAME, environment=None, description=\"Real-time fraud detection\")\n",
    "print(f\"Flow created: {FLOW_NAME}\")\n",
    "\n",
    "# Stage 1: Kafka Multitopic Consumer\n",
    "kafka_multitopic_consumer_1 = flow.add_stage(\"Kafka Multitopic Consumer\")\n",
    "kafka_multitopic_consumer_1.max_batch_size_in_records = 1\n",
    "kafka_multitopic_consumer_1.field_path_to_regex_group_mapping = [{'fieldPath': '/', 'group': 1}]\n",
    "kafka_multitopic_consumer_1.broker_uri = KAFKA_BROKER\n",
    "kafka_multitopic_consumer_1.topic_list = [KAFKA_TOPIC]\n",
    "kafka_multitopic_consumer_1.import_sheets = ['']\n",
    "kafka_multitopic_consumer_1.data_format = 'JSON'\n",
    "kafka_multitopic_consumer_1.schema_registry_urls = ['']\n",
    "print(\"  Stage 1: Kafka Consumer configured\")\n",
    "\n",
    "# Stage 2: JDBC Lookup (calculate velocity)\n",
    "jdbc_lookup_1 = flow.add_stage(\"JDBC Lookup\")\n",
    "jdbc_lookup_1.password = \"***\"\n",
    "jdbc_lookup_1.sql_query = \"\"\"WITH prev AS (\n",
    "    SELECT location, timestamp  \n",
    "    FROM transactions \n",
    "    WHERE account_id = ${record:value('/account_id')} AND suspicious = FALSE\n",
    "    ORDER BY transaction_id DESC \n",
    "    LIMIT 1\n",
    ")\n",
    "SELECT  \n",
    "    GEOGRAPHY_DISTANCE(prev.location, \\\"${record:value('/location')}\\\") as delta_x,  \n",
    "    TIMESTAMPDIFF(SECOND, prev.timestamp, \\\"${record:value('/timestamp')}\\\") as delta_t \n",
    "FROM prev;\"\"\"\n",
    "jdbc_lookup_1.username = \"***\"\n",
    "jdbc_lookup_1.jdbc_connection_string = \"jdbc:singlestore://10.89.0.2:3306/finance\"\n",
    "print(\"  Stage 2: JDBC Lookup configured\")\n",
    "\n",
    "# Stage 3: Expression Evaluator (flag suspicious transactions)\n",
    "expression_evaluator_1 = flow.add_stage(\"Expression Evaluator\")\n",
    "expression_evaluator_1.field_attribute_expressions = [{'fieldToSet': '/'}]\n",
    "expression_evaluator_1.field_expressions = [{'fieldToSet': '/suspicious', 'expression': f\"${{record:exists('/delta_x') and record:exists('/delta_t') and record:value('/delta_x') / record:value('/delta_t') > {VELOCITY_THRESHOLD}}}\"}]\n",
    "expression_evaluator_1.header_attribute_expressions = [{}]\n",
    "print(\"  Stage 3: Expression Evaluator configured\")\n",
    "\n",
    "# Stage 4: Elasticsearch (for monitoring/dashboards)\n",
    "elasticsearch_1 = flow.add_stage(\"Elasticsearch\")\n",
    "elasticsearch_1.index = \"transactions\"\n",
    "elasticsearch_1.http_urls = \"http://10.89.0.196:9200\"\n",
    "print(\"  Stage 4: Elasticsearch configured\")\n",
    "\n",
    "# Stage 5: SingleStore (for persistent storage)\n",
    "singlestore_1 = flow.add_stage(\"SingleStore\")\n",
    "singlestore_1.password = \"***\"\n",
    "singlestore_1.schema_name = \"finance\"\n",
    "singlestore_1.data_sqlstate_codes = ['']\n",
    "singlestore_1.username = \"***\"\n",
    "singlestore_1.table_name = \"transactions\"\n",
    "singlestore_1.jdbc_connection_string = \"jdbc:singlestore://10.89.0.2:3306/finance\"\n",
    "print(\"  Stage 5: SingleStore configured\")\n",
    "\n",
    "print(\"\\nAll stages configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Connect Stages and Save Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting stages...\")\n",
    "\n",
    "kafka_multitopic_consumer_1.connect_output_to(jdbc_lookup_1)\n",
    "print(\"  Kafka Consumer -> JDBC Lookup\")\n",
    "\n",
    "jdbc_lookup_1.connect_output_to(expression_evaluator_1)\n",
    "print(\"  JDBC Lookup -> Expression Evaluator\")\n",
    "\n",
    "expression_evaluator_1.connect_output_to(elasticsearch_1)\n",
    "print(\"  Expression Evaluator -> Elasticsearch\")\n",
    "\n",
    "expression_evaluator_1.connect_output_to(singlestore_1)\n",
    "print(\"  Expression Evaluator -> SingleStore\")\n",
    "\n",
    "print(\"\\nAll stages connected\")\n",
    "\n",
    "project.update_flow(flow)\n",
    "print(\"\\nFlow saved to watsonx.data Integration\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUCCESS! Streaming flow created\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFlow '{FLOW_NAME}' is now available in your project.\")\n",
    "print(\"Go to the UI to view, configure, or execute the flow.\")\n",
    "print(\"\\nhttps://ca-tor.dai.cloud.ibm.com/df/home?context=df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab Environment",
   "language": "python",
   "name": "lab-env-brew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
